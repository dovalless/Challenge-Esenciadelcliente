# ğŸ¯ Challenge: Esencia del Cliente - AnÃ¡lisis y SegmentaciÃ³n con Machine Learning

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Pandas](https://img.shields.io/badge/Pandas-1.3+-green.svg)](https://pandas.pydata.org/)
[![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.0+-orange.svg)](https://scikit-learn.org/)
[![Google Colab](https://img.shields.io/badge/Google-Colab-yellow.svg)](https://colab.research.google.com/)
[![Kaggle Dataset](https://img.shields.io/badge/Dataset-Kaggle-20BEFF.svg)](https://www.kaggle.com/datasets/ramjasmaurya/medias-cost-prediction-in-foodmart)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

<div align="center">

![Logo Alura](https://www.aluracursos.com/assets/img/challenges/logos/challenges-logo-data.1712144089.svg)

**Segundo proyecto del Bootcamp de Data Science - Alura Latam**

[ğŸ“Š Dataset](#-dataset) â€¢
[ğŸš€ Inicio RÃ¡pido](#-inicio-rÃ¡pido) â€¢
[ğŸ“ˆ MetodologÃ­a](#-metodologÃ­a) â€¢
[ğŸ” Resultados](#-resultados) â€¢
[ğŸ‘¨â€ğŸ’» Autor](#-autor)

</div>

---

## ğŸ“‹ DescripciÃ³n del Proyecto

Este proyecto se enfoca en **comprender profundamente el comportamiento de los clientes** mediante tÃ©cnicas avanzadas de anÃ¡lisis de datos y machine learning. Utilizando algoritmos de clustering (agrupamiento), identificamos patrones significativos que permiten segmentar clientes en grupos homogÃ©neos, facilitando estrategias de marketing personalizadas y mejorando la experiencia del cliente.

### ğŸ¯ Objetivos Principales

- ğŸ” **AnÃ¡lisis Exploratorio Profundo**: Visualizar y entender las caracterÃ­sticas clave de los clientes
- ğŸ¤– **SegmentaciÃ³n Inteligente**: Aplicar K-Means para agrupar clientes con comportamientos similares
- ğŸ“Š **ReducciÃ³n de Dimensionalidad**: Implementar PCA para optimizar el anÃ¡lisis
- âœ… **ValidaciÃ³n Rigurosa**: Evaluar la calidad de los clusters con mÃºltiples mÃ©tricas
- ğŸ’¡ **Insights Accionables**: Generar recomendaciones estratÃ©gicas basadas en datos

### ğŸ’¼ Impacto del Proyecto

La importancia de este anÃ¡lisis radica en su capacidad para **transformar datos en informaciÃ³n accionable**. Al identificar y comprender diferentes segmentos de clientes, las empresas pueden:

- âœ… Desarrollar estrategias de marketing mÃ¡s efectivas y personalizadas
- âœ… Optimizar la asignaciÃ³n de recursos y presupuestos publicitarios
- âœ… Mejorar la retenciÃ³n y satisfacciÃ³n del cliente
- âœ… Incrementar las ventas mediante ofertas dirigidas
- âœ… Tomar decisiones basadas en evidencia cuantitativa

---

## ğŸ“Š Dataset

### Fuente de Datos

Los datos fueron extraÃ­dos del conjunto de datos **"Media's Cost Prediction in Foodmart"** disponible en Kaggle:

ğŸ”— **[Dataset en Kaggle](https://www.kaggle.com/datasets/ramjasmaurya/medias-cost-prediction-in-foodmart)**

### CaracterÃ­sticas del Dataset

| CaracterÃ­stica | Detalle |
|----------------|---------|
| **Origen** | Foodmart - Cadena de supermercados |
| **Tipo de datos** | Costos de medios, ventas y demografÃ­a de clientes |
| **Variables** | CategÃ³ricas y numÃ©ricas (mixtas) |
| **Idioma original** | InglÃ©s (traducido al espaÃ±ol) |
| **Formato** | CSV |

### Variables Principales Analizadas

```
ğŸ“Œ Variables DemogrÃ¡ficas:
   â€¢ Escolaridad
   â€¢ OcupaciÃ³n
   â€¢ GÃ©nero
   â€¢ Estado Civil
   â€¢ NÃºmero de Hijos

ğŸ“Œ Variables EconÃ³micas:
   â€¢ Ingresos Anuales
   â€¢ Tipo de Miembro
   â€¢ CategorÃ­a de Alimentos
   â€¢ Tipo de Producto
```

---

## ğŸ› ï¸ TecnologÃ­as y Herramientas

### Stack TecnolÃ³gico

```python
# AnÃ¡lisis de Datos
pandas >= 1.3.0
numpy >= 1.19.0

# VisualizaciÃ³n
matplotlib >= 3.3.0
seaborn >= 0.11.0

# Machine Learning
scikit-learn >= 1.0.0

# Ambiente de Desarrollo
Google Colab
Google Drive (almacenamiento)
```

### TÃ©cnicas de Machine Learning Implementadas

| TÃ©cnica | PropÃ³sito |
|---------|-----------|
| **K-Means** | Algoritmo de clustering no supervisado |
| **PCA** | ReducciÃ³n de dimensionalidad |
| **StandardScaler** | NormalizaciÃ³n de datos |
| **One-Hot Encoding** | CodificaciÃ³n de variables categÃ³ricas |

### MÃ©tricas de ValidaciÃ³n

- ğŸ¯ **Silhouette Score** (objetivo: â‰¥ 0.50)
- ğŸ“‰ **Davies-Bouldin Index** (objetivo: â‰¤ 0.75)
- ğŸ“ˆ **Calinski-Harabasz Index** (maximizar)

---

## ğŸš€ Inicio RÃ¡pido

### Requisitos Previos

- âœ… Cuenta de Gmail (para acceder a Google Colab)
- âœ… Acceso a Google Drive
- âœ… Descarga del dataset desde Kaggle

### InstalaciÃ³n Paso a Paso

#### 1ï¸âƒ£ Configurar Google Colab

```bash
# 1. Accede a Google Colab
https://colab.research.google.com/

# 2. Crea un nuevo notebook
Archivo â†’ Nuevo Notebook

# 3. Renombra el notebook
"La esencia del cliente 1" (o el nombre de tu preferencia)
```

#### 2ï¸âƒ£ Conectar con Google Drive

```python
# Montar Google Drive en Colab
from google.colab import drive
drive.mount('/content/drive')
```

#### 3ï¸âƒ£ Descargar y Preparar el Dataset

1. **Descarga** el dataset desde [Kaggle](https://www.kaggle.com/datasets/ramjasmaurya/medias-cost-prediction-in-foodmart)
2. **Crea** un directorio en Google Drive: `Mi unidad/Datasets/Challenge-Cliente/`
3. **Sube** el archivo CSV al directorio creado

#### 4ï¸âƒ£ Cargar Dependencias

```python
# Importar librerÃ­as necesarias
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# Configurar visualizaciones
%matplotlib inline
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 6)
```

---

## ğŸ“ˆ MetodologÃ­a

### Pipeline del Proyecto

```
ğŸ“¥ Carga de Datos
    â†“
ğŸŒ TraducciÃ³n al EspaÃ±ol
    â†“
ğŸ” ExploraciÃ³n Visual (EDA)
    â†“
ğŸ”§ Preprocesamiento
    â†“
ğŸ“Š Feature Engineering
    â†“
ğŸ¤– Clustering (K-Means)
    â†“
âœ… ValidaciÃ³n de Clusters
    â†“
ğŸ“ AnÃ¡lisis e Insights
```

---

### Fase 1: Carga y TraducciÃ³n de Datos

#### Cargar Dataset desde Google Drive

```python
# Ruta al dataset en Google Drive
ruta_dataset = '/content/drive/MyDrive/Datasets/Challenge-Cliente/data.csv'

# Cargar datos
datos_raw = pd.read_csv(ruta_dataset)

# Vista preliminar
print(datos_raw.head())
print(f"Dimensiones: {datos_raw.shape}")
```

#### TraducciÃ³n al EspaÃ±ol

```python
# Diccionario de traducciÃ³n (ejemplo)
traduccion_columnas = {
    'Education': 'Escolaridad',
    'Occupation': 'Ocupacion',
    'Member': 'Miembro',
    'Gender': 'Genero',
    'Marital_Status': 'Estado_Civil',
    'Num_Children': 'Num_Hijos',
    'Annual_Income': 'Ingresos_Anuales',
    'Food_Category': 'Categoria_Alimentos',
    'Type': 'Tipo'
}

# Aplicar traducciÃ³n
datos_raw.rename(columns=traduccion_columnas, inplace=True)

# Exportar versiÃ³n traducida
datos_raw.to_csv('/content/drive/MyDrive/Datasets/Challenge-Cliente/datos_traducidos.csv', index=False)
```

---

### Fase 2: ExploraciÃ³n Visual de Datos (EDA)

#### AnÃ¡lisis EstadÃ­stico Descriptivo

```python
# EstadÃ­sticas descriptivas
print(datos_raw.describe())

# InformaciÃ³n general
print(datos_raw.info())

# Valores nulos
print(datos_raw.isnull().sum())
```

#### Visualizaciones Clave

**1. DistribuciÃ³n de Variables NumÃ©ricas**

```python
# Histograma de Ingresos Anuales
plt.figure(figsize=(10, 6))
sns.histplot(datos_raw['Ingresos_Anuales'], kde=True, bins=30)
plt.title('DistribuciÃ³n de Ingresos Anuales de Clientes')
plt.xlabel('Ingresos Anuales ($)')
plt.ylabel('Frecuencia')
plt.show()
```

**2. AnÃ¡lisis de Variables CategÃ³ricas**

```python
# DistribuciÃ³n por GÃ©nero
plt.figure(figsize=(8, 5))
sns.countplot(data=datos_raw, x='Genero', palette='viridis')
plt.title('DistribuciÃ³n de Clientes por GÃ©nero')
plt.show()
```

**3. CorrelaciÃ³n entre Variables**

```python
# Matriz de correlaciÃ³n (solo variables numÃ©ricas)
plt.figure(figsize=(12, 8))
sns.heatmap(datos_raw.select_dtypes(include=[np.number]).corr(), 
            annot=True, cmap='coolwarm', center=0)
plt.title('Matriz de CorrelaciÃ³n')
plt.show()
```

**ğŸ“ Ejemplo de Observaciones:**

> *"Los clientes con mayor escolaridad tienden a tener ingresos anuales mÃ¡s altos. Existe una correlaciÃ³n positiva entre el nÃºmero de hijos y el gasto en categorÃ­a de alimentos."*

---

### Fase 3: Preprocesamiento y Feature Engineering

#### CodificaciÃ³n de Variables CategÃ³ricas

**OpciÃ³n 1: One-Hot Encoding**

```python
# Variables categÃ³ricas a codificar
categoricas = ['Escolaridad', 'Ocupacion', 'Genero', 'Estado_Civil']

# Aplicar One-Hot Encoding
datos_encoded = pd.get_dummies(datos_raw, columns=categoricas, drop_first=True)
```

**OpciÃ³n 2: Label Encoding (Ordinal)**

```python
# Ejemplo: Escolaridad con orden jerÃ¡rquico
escolaridad_map = {
    'Primaria': 1,
    'Secundaria': 2,
    'Universidad': 3,
    'Posgrado': 4
}

datos_raw['Escolaridad_Num'] = datos_raw['Escolaridad'].map(escolaridad_map)
```

#### SelecciÃ³n de Features Relevantes

```python
# Seleccionar entre 6 y 12 atributos mÃ¡s relevantes
features_seleccionadas = [
    'Escolaridad_Num',
    'Ingresos_Anuales',
    'Num_Hijos',
    'Edad',
    'Gasto_Total',
    'Frecuencia_Compra',
    'Categoria_Alimentos_Num',
    'Genero_M'  # si se usÃ³ One-Hot
]

X = datos_raw[features_seleccionadas]
```

#### EstandarizaciÃ³n de Datos

```python
# Instanciar StandardScaler
scaler = StandardScaler()

# Ajustar y transformar
X_std = scaler.fit_transform(X)

print(f"Forma de X_std: {X_std.shape}")
# Output: (n_muestras, n_features)
```

---

### Fase 4: Clustering con K-Means

#### DeterminaciÃ³n del NÃºmero Ã“ptimo de Clusters

**MÃ©todo del Codo (Elbow Method)**

```python
# Calcular inercia para diferentes nÃºmeros de clusters
inercias = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_std)
    inercias.append(kmeans.inertia_)

# GrÃ¡fico del codo
plt.figure(figsize=(10, 6))
plt.plot(K_range, inercias, marker='o', linewidth=2)
plt.xlabel('NÃºmero de Clusters (k)')
plt.ylabel('Inercia')
plt.title('MÃ©todo del Codo para Determinar k Ã“ptimo')
plt.grid(True)
plt.show()
```

#### ValidaciÃ³n con MÃºltiples MÃ©tricas

```python
# Evaluar de 3 a 10 clusters
resultados = []

for k in range(3, 11):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_std)
    
    # Calcular mÃ©tricas
    silhouette = silhouette_score(X_std, labels)
    davies_bouldin = davies_bouldin_score(X_std, labels)
    calinski = calinski_harabasz_score(X_std, labels)
    
    resultados.append({
        'k': k,
        'Silhouette': silhouette,
        'Davies-Bouldin': davies_bouldin,
        'Calinski-Harabasz': calinski
    })

# Crear DataFrame con resultados
df_metricas = pd.DataFrame(resultados)
print(df_metricas)
```

**Criterios de SelecciÃ³n:**

- âœ… **Silhouette** â‰¥ 0.50 (mayor es mejor)
- âœ… **Davies-Bouldin** â‰¤ 0.75 (menor es mejor)
- âœ… **Calinski-Harabasz**: maximizar

---

### Fase 5: ValidaciÃ³n de Estructura y Estabilidad

#### 1ï¸âƒ£ ValidaciÃ³n de Estructura (Baseline Aleatorio)

```python
# Generar datos aleatorios con la misma forma que X_std
random_data = np.random.rand(*X_std.shape)

# Aplicar KMeans al baseline
k_optimo = 4  # ejemplo: mejor k encontrado
kmeans_random = KMeans(n_clusters=k_optimo, random_state=42)
labels_random = kmeans_random.fit_predict(random_data)

# Calcular mÃ©tricas en baseline
sil_random = silhouette_score(random_data, labels_random)
db_random = davies_bouldin_score(random_data, labels_random)
ch_random = calinski_harabasz_score(random_data, labels_random)

print(f"Baseline Aleatorio - Silhouette: {sil_random:.3f}")
print(f"Datos Reales - Silhouette: {silhouette_score(X_std, labels):.3f}")
# Asegurar que X_std >> random_data
```

#### 2ï¸âƒ£ ValidaciÃ³n de Estabilidad (Cross-Validation)

```python
# Dividir X_std en 5 partes iguales
splits = np.array_split(X_std, 5)

metricas_estabilidad = []

for i, split in enumerate(splits):
    kmeans_split = KMeans(n_clusters=k_optimo, random_state=42)
    labels_split = kmeans_split.fit_predict(split)
    
    sil = silhouette_score(split, labels_split)
    db = davies_bouldin_score(split, labels_split)
    ch = calinski_harabasz_score(split, labels_split)
    
    metricas_estabilidad.append({
        'Split': i+1,
        'Silhouette': sil,
        'Davies-Bouldin': db,
        'Calinski-Harabasz': ch
    })

df_estabilidad = pd.DataFrame(metricas_estabilidad)

# Calcular variaciÃ³n porcentual
variacion_sil = df_estabilidad['Silhouette'].std() / df_estabilidad['Silhouette'].mean() * 100
print(f"VariaciÃ³n en Silhouette: {variacion_sil:.2f}%")
# Objetivo: variaciÃ³n < 5%
```

---

### Fase 6: AsignaciÃ³n de Clusters al Dataset

```python
# Instanciar modelo final con k Ã³ptimo
kmeans_final = KMeans(n_clusters=k_optimo, random_state=42, n_init=10)

# Ajustar y predecir
datos_raw['cluster'] = kmeans_final.fit_predict(X_std)

# Verificar distribuciÃ³n de clusters
print(datos_raw['cluster'].value_counts().sort_index())
```

---

### Fase 7: AnÃ¡lisis e InterpretaciÃ³n de Clusters

#### VisualizaciÃ³n de Clusters

**1. GrÃ¡fico de DispersiÃ³n 2D**

```python
# Seleccionar dos variables clave
plt.figure(figsize=(12, 7))
sns.scatterplot(data=datos_raw, 
                x='Ingresos_Anuales', 
                y='Gasto_Total',
                hue='cluster', 
                palette='Set2', 
                s=100, 
                alpha=0.7)
plt.title('SegmentaciÃ³n de Clientes: Ingresos vs Gasto Total')
plt.xlabel('Ingresos Anuales ($)')
plt.ylabel('Gasto Total ($)')
plt.legend(title='Cluster')
plt.show()
```

**2. VisualizaciÃ³n 3D (con PCA)**

```python
from mpl_toolkits.mplot3d import Axes3D

# Reducir a 3 componentes principales
pca = PCA(n_components=3)
X_pca = pca.fit_transform(X_std)

# GrÃ¡fico 3D
fig = plt.figure(figsize=(14, 10))
ax = fig.add_subplot(111, projection='3d')

scatter = ax.scatter(X_pca[:, 0], 
                     X_pca[:, 1], 
                     X_pca[:, 2],
                     c=datos_raw['cluster'], 
                     cmap='viridis', 
                     s=50, 
                     alpha=0.6)

ax.set_xlabel('Componente Principal 1')
ax.set_ylabel('Componente Principal 2')
ax.set_zlabel('Componente Principal 3')
ax.set_title('Clusters en Espacio PCA 3D')
plt.colorbar(scatter, label='Cluster')
plt.show()
```

#### Perfiles de Clusters

```python
# AnÃ¡lisis estadÃ­stico por cluster
perfiles = datos_raw.groupby('cluster')[features_seleccionadas].mean()
print(perfiles)

# Visualizar perfiles
perfiles.T.plot(kind='bar', figsize=(14, 8), colormap='tab10')
plt.title('Perfil Promedio de Cada Cluster')
plt.ylabel('Valor Promedio Estandarizado')
plt.xlabel('Features')
plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1))
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()
```

---

## ğŸ” Resultados

### Ejemplo de DescripciÃ³n de Clusters

#### ğŸ“Š **Cluster 0: Clientes Premium**
- **CaracterÃ­sticas**:
  - Ingresos anuales superiores a $80,000
  - Alta escolaridad (Universidad/Posgrado)
  - Gasto elevado en productos premium
  - Edad promedio: 40-55 aÃ±os
  
- **Estrategia sugerida**:
  - Programa de lealtad exclusivo
  - ComunicaciÃ³n personalizada de productos premium
  - Eventos VIP

#### ğŸ“Š **Cluster 1: Familias JÃ³venes**
- **CaracterÃ­sticas**:
  - Ingresos medios ($40,000 - $60,000)
  - NÃºmero de hijos: 2-3
  - Mayor gasto en categorÃ­a de alimentos
  - Edad promedio: 30-40 aÃ±os
  
- **Estrategia sugerida**:
  - Promociones familiares
  - Descuentos en productos infantiles
  - Programas de ahorro

#### ğŸ“Š **Cluster 2: Compradores Ocasionales**
- **CaracterÃ­sticas**:
  - Ingresos bajos-medios (< $40,000)
  - Frecuencia de compra baja
  - Sensibilidad al precio
  - Sin hijos o 1 hijo
  
- **Estrategia sugerida**:
  - Cupones y descuentos
  - ComunicaciÃ³n de ofertas especiales
  - Programa de puntos

#### ğŸ“Š **Cluster 3: Seniors Estables**
- **CaracterÃ­sticas**:
  - Ingresos medios-altos por jubilaciÃ³n
  - Edad > 60 aÃ±os
  - Compras regulares pero moderadas
  - Prefieren productos de calidad
  
- **Estrategia sugerida**:
  - Servicio personalizado
  - Productos de salud y bienestar
  - Facilidades de entrega a domicilio

---

## ğŸ“š Recursos y Referencias

### DocumentaciÃ³n Oficial

- [Scikit-learn - K-Means](https://scikit-learn.org/stable/modules/clustering.html#k-means)
- [Scikit-learn - PCA](https://scikit-learn.org/stable/modules/decomposition.html#pca)
- [Pandas Documentation](https://pandas.pydata.org/docs/)

### Tutoriales Recomendados

- [K-Means Clustering - Antonio Richaud](https://antonio-richaud.com/blog/archivo/publicaciones/12-k-means.html)
- [PCA (AnÃ¡lisis de Componentes Principales) - Antonio Richaud](https://antonio-richaud.com/blog/archivo/publicaciones/29-pca.html)
- [Google Colab - GuÃ­a Oficial](https://colab.research.google.com/notebooks/intro.ipynb)

### Papers y ArtÃ­culos

- **K-Means Clustering**: MacQueen, J. (1967). "Some methods for classification and analysis of multivariate observations"
- **PCA**: Pearson, K. (1901). "On lines and planes of closest fit to systems of points in space"

---

## ğŸ¤ Contribuciones

Â¡Las contribuciones son bienvenidas! Si deseas mejorar este proyecto:

1. **Fork** el repositorio
2. Crea tu **feature branch** (`git checkout -b feature/MejoraMagica`)
3. **Commit** tus cambios (`git commit -m 'AÃ±ade nueva mÃ©trica de validaciÃ³n'`)
4. **Push** a la rama (`git push origin feature/MejoraMagica`)
5. Abre un **Pull Request**

### Ideas de ContribuciÃ³n

- ğŸ“Š Implementar otros algoritmos de clustering (DBSCAN, Hierarchical)
- ğŸ¨ Mejorar visualizaciones con Plotly (interactividad)
- ğŸ“ˆ AÃ±adir anÃ¡lisis de series temporales
- ğŸ§ª Integrar pruebas unitarias
- ğŸ“ Traducir documentaciÃ³n a otros idiomas

---

## ğŸ‘¨â€ğŸ’» Autor

<div align="center">

**Darwin Manuel Ovalles Cesar**

<p align="center">
<a href="https://www.linkedin.com/in/darwin-manuel-ovalles-cesar-dev" target="_blank">
<img align="center" src="https://raw.githubusercontent.com/rahuldkjain/github-profile-readme-generator/master/src/images/icons/Social/linked-in-alt.svg" alt="LinkedIn - Darwin Ovalles" height="40" width="50" />
</a>
</p>

ğŸ’¼ **LinkedIn**: [darwin-manuel-ovalles-cesar-dev](https://www.linkedin.com/in/darwin-manuel-ovalles-cesar-dev)  
ğŸŒ **GitHub**: [@dovalless](https://github.com/dovalless)  
ğŸ“§ **Email**: Disponible en LinkedIn

---

*"Este proyecto es una contribuciÃ³n con todo el amor del mundo para aquellos que buscan formarse en el fascinante Ã¡mbito de la Ciencia de Datos. Espero que mi trabajo pueda servir como una guÃ­a y recurso valioso para cualquier persona interesada en mejorar sus habilidades y conocimientos en esta Ã¡rea."*

**#aluraChallengeEsenciaDelCliente**

</div>

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Consulta el archivo `LICENSE` para mÃ¡s detalles.

---

## ğŸ† Insignia del Challenge

<div align="center">

![Insignia Challenge](./imagenes/medallita.png)

**Bootcamp de Data Science - Alura Latam**

</div>

---

## ğŸ™ Agradecimientos

- **Alura Latam** - Por el excelente programa de formaciÃ³n en Data Science
- **Kaggle** - Por proporcionar datasets de calidad para practicar
- **Comunidad Open Source** - Por las herramientas y librerÃ­as utilizadas
- **Antonio Richaud** - Por los excelentes tutoriales de K-Means y PCA

---

<div align="center">

**â­ Si este proyecto te resultÃ³ Ãºtil, considera darle una estrella en GitHub â­**

**ğŸš€ Â¡Feliz anÃ¡lisis de datos! ğŸš€**

---

Desarrollado con ğŸ’š y â˜• por [Darwin Ovalles](https://www.linkedin.com/in/darwin-manuel-ovalles-cesar-dev)

</div>
